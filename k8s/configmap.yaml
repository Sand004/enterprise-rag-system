apiVersion: v1
kind: ConfigMap
metadata:
  name: rag-config
  namespace: rag-system
data:
  # Vector Database Configuration
  VECTOR_DB_TYPE: "qdrant"
  QDRANT_URL: "http://qdrant-service:6333"
  
  # PostgreSQL Configuration
  POSTGRES_HOST: "postgres-service"
  POSTGRES_PORT: "5432"
  POSTGRES_DB: "rag_production"
  
  # Redis Configuration
  REDIS_URL: "redis://redis-service:6379"
  
  # LLM Configuration
  LLM_PROVIDER: "vllm"
  VLLM_ENDPOINT: "http://vllm-service:8000"
  VLLM_MODEL: "meta-llama/Llama-3.3-70B-Instruct"
  VLLM_QUANTIZATION: "awq"
  
  # Storage Configuration
  S3_ENDPOINT: "http://minio-service:9000"
  S3_BUCKET_NAME: "rag-documents"
  
  # Application Settings
  MAX_CHUNK_SIZE: "1024"
  CHUNK_OVERLAP: "256"
  EMBEDDING_MODEL: "text-embedding-3-large"
  EMBEDDING_DIMENSION: "3072"
  
  # Feature Flags
  ENABLE_GRAPHRAG: "true"
  ENABLE_MULTI_AGENT: "true"
  ENABLE_CACHING: "true"
  ENABLE_AUDIT_LOGGING: "true"
  ENABLE_ENCRYPTION: "true"
  
  # Performance Settings
  MAX_CONCURRENT_REQUESTS: "100"
  REQUEST_TIMEOUT_SECONDS: "300"
  CACHE_TTL_SECONDS: "3600"
  VECTOR_SEARCH_TOP_K: "10"
  RERANKING_TOP_K: "5"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: rag-config-files
  namespace: rag-system
data:
  models.yaml: |
    embedding:
      primary:
        name: "text-embedding-3-large"
        provider: "openai"
        dimension: 3072
        max_tokens: 8191
        batch_size: 100
    
    llm:
      vllm:
        primary:
          name: "meta-llama/Llama-3.3-70B-Instruct"
          quantization: "awq"
          tensor_parallel_size: 4
          max_model_len: 32768
          gpu_memory_utilization: 0.9
  
  vector_db.yaml: |
    qdrant:
      collections:
        enterprise_docs:
          vectors:
            size: 3072
            distance: "Cosine"
          optimizers:
            indexing_threshold: 20000
            memmap_threshold: 50000
          hnsw_config:
            m: 16
            ef_construct: 200
            full_scan_threshold: 10000